\documentclass{report}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath}
\usepackage{bm}
%opening
\title{}
\author{Ash}

\begin{document}
	
	\maketitle
	\thispagestyle{empty}
	\newpage
	\thispagestyle{empty}
	\tableofcontents
	\newpage
	\thispagestyle{empty}
	\listoffigures
	\newpage
	
	\chapter{Introduction}
	\textbf{Very brief background}
	Deep learning good for large data set.
	Motivation is to work with less training examples \\
	\textbf{Existing work on meta learning and continuous learning} \\
	\textbf{Gap the literature} \\
	\textbf{Describe the Problem}
	A system that can do continual learning. Refer to this doc:
	https://paper.dropbox.com/doc/Adding-classes-an-existing-classifier-RdKxXHh7M9OWbHvEvCCsV
	We want it to be scalable with respect to the number of classes
	So it should work faster than nearest neighbour based approaches for large number of classes \\
	\textbf{High level how you will solve it and why it is different from existing work} \\
	\textbf{Brief description of experimental setup} \\
	
	% TODO: Week 2
	\chapter{Background}
	% TODO: Week 2
	\section{Hand Engineered vs Learnt Features}
	Features are a general term for characteristic attributes which exist across all samples in a data-set or domain. These features were traditionally hand-engineered by machine learning experts, carefully selecting the base-components of which the data-set in question appears to comprise. \\
	A fundamental problem with hand-engineered features is that it imposes human knowledge onto a problem to be solved by a computer. Furthermore, key features for complex data such as images and video are incredibly difficult to ascertain -- especially if desiring generic, transferable features. With the rise of neural networks -- specifically CNNs, which will be discussed in detail later -- feature-learning has become the norm. This essentially takes the task of feature engineering and solves it in a data-driven manner.
	% TODO: Week 2
	\section{Supervised Learning}
	Supervised learning is a machine learning strategy whereby the target solution is presented after each training iteration. This differs from unsupervised learning in that unsupervised learning has no direct target to learn from and is used to find underlying commonalities or patterns in data. \\
	Supervised learning is the most commonly used method for image and video tasks, as typically the objective is to perform tasks where the target is well-defined. Common supervised learning tasks are \textit{image classification} - where the objective is to assign an input image a label from a fixed set of categories; \textit{localisation} - where the objective is to produce the coordinates of an object of interest from the input image; and \textit{detection} - which combines the previous two tasks. \\
	There are a multitude of supervised and unsupervised learning problems. As with almost all meta-learning strategies, I will focus primarily on the supervised task of image classification.
	% TODO: Week 2
	\section{Optimisation}
	\subsection{Loss}
	The process of optimising a machine learning system is to present it with a target of some sort -- either in the supervised or unsupervised setting -- and compute a numerical quantity called either \textit{loss} or \textit{cost}. It is the system's objective to minimise this value through some optimisation algorithm. The particular loss function is specifically chosen for the task at hand, cross-entropy being a common choice for image classification tasks. \\
	Before discussing the cross-entropy loss function, it's important to understand the outputs of a machine learning system when performing image classification. \\
	For a system that makes predictions between $N$ classes, its output is a vector of length $N$, where each of the output values $N_i$ is a a score for that class $i$ being the correct answer. \\
	As the outputs aren't normalised and thus cannot be interpreted as a true confidence measure, the outputs then normally go through a softmax function $\sigma$.
	\begin{equation} \label{softmax:1}
	\sigma(\bm{z})_i = \frac{e^{z_i}}{\sum_{k=1}^{N}e^{z_i}} \\
	\end{equation}
	The softmax function (eq \ref{softmax:1}) squashes the arbitrary scores into a vector of values such that their values are in the range $[0, 1]$ and that all components sum to $1$. The resultant values can be interpreted as the probability measures for the input image falling into each of the classes. \\
	- What loss is \\
	- Loss curve \\
	- Gradient descent \\
		- Why \\
		- How \\
	% TODO: Week 2
	\subsection{SGD}
	- Why use Sgd
	
	% TODO: Week 2
	\subsection{Backprop}
	% TODO: Week 2
	\subsection{Auto optimizers like ADAM and RMSProp}
	
	\section{Dealing with Small Training Data Sets}
	\subsection{Overfitting}
	\subsection{Transfer Learning}
	\subsection{Few-Shot Learning}
	\subsection{Meta Learning}
	\textit{Break into meta training, testing, episodes, etc.}
	\section{Continuous Learning}
	\textit{Catastrophic forgetting}
	\section{Modern Deep Learning Architectures}
	\subsection{Convolutional Neural Networks}
	\subsection{Recurrent Neural Networks}
	
	\chapter{Related Works}
	\section{Meta Learning}
	Approaches in meta learning with neural networks are generally groupd into three categories.
	\subsection{Model Based}
	\subsection{Metric Based}
	\subsection{Optimization Based}
	\section{Continuous Learning}
	
	\chapter{Proposal}
	


\end{document}
